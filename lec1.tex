\documentclass{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{amsmath,amsfonts,graphicx}
\usepackage{tikz}
\usepackage{epsfig}
\usepackage{filecontents}
\usepackage{pgfplots, pgfplotstable}
\usepackage{mathtools}
\usepackage{breqn}
%\usepackage{fourier}
\usepgfplotslibrary{statistics}
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

\newcommand{\lecture}[4]{
  \pagestyle{myheadings}
  \thispagestyle{plain}
  \setcounter{lecnum}{#1}
  \setcounter{page}{1}
  \noindent
  \begin{center}
    \framebox{
      \vbox{\vspace{2mm}
        \hbox to 6.28in { {\bf Analysis of Uncertainties
	    \hfill Fall 2014} }
        \vspace{4mm}
        \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
        \vspace{2mm}
        \hbox to 6.28in { {\it Lecturer: #3 \hfill Notes by: #4} }
        \vspace{2mm}}
    }
  \end{center}
  \markboth{Lecture #1: #2}{Lecture #1: #2}
  \vspace*{4mm}
}

\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
    {[\arabic{equation}]}{\usecounter{equation}
      \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
      \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\newcommand\E{\mathbb{E}}


\begin{document}
\lecture{1}{20 October 2014}{Prof. Kate Scholberg}{Douglas Davis}
\section{Introduction}
Think about the value for Newton's gravitational constant $G$; the current accepted value in literature is something around (call it $G_{\text{m1}}$, this doesn't have to be correct we're just making a point).
$$ G_{\text{m1}} = (6.674215\pm0.000092) \times 10^{11} \text{ m}^3\text{ kg}^{-1}\text{ s}^{-2}. $$
Now imagine we perform some experiments where we measure $G$. Imagine we measure a a new value, call it $G_{\text{m2}}$,
$$ G_{\text{m2}} = (6.674400\pm0.000010) \times 10^{11} \text{ m}^3\text{ kg}^{-1}\text{ s}^{-2}. $$
And we measure another value, $G_{\text{m3}}$,
$$ G_{\text{m3}} = (6.6741\pm0.0221) \times 10^{11} \text{ m}^3\text{ kg}^{-1}\text{ s}^{-2}. $$
If we look closely at the values and the uncertainties, we see that the value $G_{\text{m2}}$ is in disagreement with $G_{\text{m1}}$, because the values do not lie within the other value's uncertainty. This means some further investigation is needed! If we look at $G_{\text{m3}}$, we see that it is in agreement with both $G_{\text{m1}}$ and $G_{\text{m1}}$, but the uncertainty is so large that this measurement really did nothing for the advancement of science. This can be seen graphically in Figure~\ref{fig:G}.
\begin{figure}[h!]
  \centerline{
    \begin{tikzpicture}
      \draw [->] (0,0) -- (4,0);
      \draw [->] (0,0) -- (0,3);
      \node at (2,-.85) {Measurements};
      \node at (-.5,1.5) {$G$};
      \node at (1,-.35) {1};
      \node at (2,-.35) {2};
      \node at (3,-.35) {3};
      \draw [|-|,dashed,blue] (1,.9) -- (1,1.9);
      \draw [|-|,dashed,blue] (2,2.2)-- (2,2.6);
      \draw [<->,dashed,blue] (3,.1) -- (3,2.9);
      \draw [fill=red] (1,1.4) circle (2pt);
      \draw [fill=red] (2,2.4) circle (2pt);
      \draw [fill=red] (3,1.0) circle (2pt);
    \end{tikzpicture}
  }
  \caption{Three measurements for $G$, the values are red bullets, and the uncertainties are shown with blue dashed error bars}
  \label{fig:G}
\end{figure}
The point of this example is to show that uncertainties seriously matter. When interpreting results and planning new experiments, a scientist must always be thinking about uncertainties. Why build an experiment if the expected uncertainty in the measurement will yield nothing better than what is already known? Or, what does this new measurement mean that is it so much more constrained than the previous measurement? Is this method so great we should expand it elsewhere, and why is it so much better?

\section{Important Definitions}
Our first two important definitions are \emph{statistical} and \emph{systematic uncertainties}. A \emph{statistical uncertainty} is a random uncertainty that arises simply from random fluctuations in the data. As the sample size of the data approaches infinity ($N \rightarrow \infty$), the statistical uncertainty approaches 0. A \emph{systematic uncertainty} comes from a lack of knowledge about the experimental apparatus/measurement. An example would be the possibly of an inexact measurement from an imperfect ruler. Systematic uncertainties can also have some randomness, if an apparatus is affected by temperature change, the random fluctuations of temperature in the room cause systematic uncertainties because of the effect on the apparatus. To quote the systematic and statistical uncertainties, results are often written in the form:
\begin{align}
  x \pm \Delta x\,(\text{sys.}) \pm \Delta x\,(\text{stat.}),
\end{align}
with the two values separate. As the amount of data increases, the statistical uncertainty always decreases. The same cannot be said for systematic. In theory, studying systematic uncertainties more thoroughly may lead to a decrease, but it is possible that a better understanding of the systematics may cause an increase in the uncertainty.
\noindent Our next definition is a \emph{sample}. The \emph{sample} is the collection of measurements. If we want to measure some value $x$, then the sample is simply:
$$x_1,\,x_2,\,x_3,\,...\,,\,x_N,$$
where $x_i$ is just the $i$th measurement. An infinite number of measurements would yield the sample's \emph{parent distribution}, which we denote with $P(x)$. Figure~\ref{fig:sample,parent} shows an example of a histogrammed sample with it's parent distribution. As $N\rightarrow\infty$, the shape of the histogram would approach the shape of $P(x)$.

\begin{figure}[h!]
  \centerline{
    \input{tikz/histo.tex}
  }
  \caption{A sample distribution in histogram form overlayed by its parent distribution.}
  \label{fig:sample,parent}
\end{figure}

\noindent Now we will define some characteristics of the sample and the parent distribution. First, the \emph{mean of the sample}, which we denote with $\overline{x}$:
\begin{align}
  \overline{x} = \frac{1}{N}\sum_i^N x_i.
  \label{eq:sample_mean}
\end{align}
The \emph{mean of the parent distribution}, denoted by $\mu$:
\begin{align}
  \mu = \lim_{N\rightarrow\infty}\frac{1}{N}\sum_i^N x_i.
  \label{eq:parent_mean}
\end{align}
To quantify the spread we may first think to use the \emph{deviation}, denoted by $d$,
\begin{align}
  d_i = x_i - \mu,
  \label{eq:deviation}
\end{align}
and take the limit:
\begin{align}
  \lim_{N\rightarrow\infty} \frac{1}{N}\sum_i^N d_i = \lim_{N\rightarrow\infty} \frac{1}{N}\sum_i^N (x_i-\mu),
\end{align}
but because $d_i$ can be positive or negative, this limit tends to zero. We may also think to take the absolute value of the deviation, $|d_i|$, but this math turns out to be pretty bad. Instead we go with something called the \emph{variance}, which we denote with $\sigma^2$:
\begin{align}
  \sigma^2 = \lim_{N\rightarrow\infty}\frac{1}{N}\sum_i^N(x_i - \mu)^2.
  \label{eq:variance}
\end{align}
Which can also be written as:
\begin{align}
  \sigma^2 = \lim_{N\rightarrow\infty}\left[\frac{1}{N}\sum_i^N x_i^2\right] - \mu^2
  \label{eq:variance2}
\end{align}
In real world experiments, we probably do not know the parent mean, so we take $\mu\rightarrow \overline{x}$. Now we define the \emph{sample variance}, denoted by $s^2$:
\begin{align}
  s^2 = \frac{1}{N-1}\sum_i^N\left(x_i-\overline{x}\right)^2
  \label{eq:sample_variance}
\end{align}
Notice the difference between the fraction in equation~\ref{eq:variance} and equation~\ref{eq:sample_variance}, we change from $1/N$ to $1/(N+1)$. The reason for this can be worked out mathematically (ref), but we will just explain the following reason. Imagine taking only 1 measurement ($N=1$). Equation~\ref{eq:variance} would yield a variance of only 1. This does not make sense for a single measurement, the outcome using equation~\ref{eq:sample_variance} would be an undefined variance, which does make sense. There is no variance in a single measurement. The sample variance defined in equation~\ref{eq:sample_variance} is known as an \emph{unbiased estimator}. Closing side note: for practical purposes many scientists use the notation of $\sigma^2$ for the sample variance and $\mu$ for the sample mean.


\section{Propagation of uncertainties}
Let's suppose the value we are attempting to measure, $x$, is a function of $u$:
\begin{align}
  x = f(u) = u^2
  \label{eq:x_func}
\end{align}
We know that our measurement, $x$, will have some uncertainty (which we will denote as $\delta x$ or $\sigma_x$), and our independent variable must also have some uncertainty: $\delta u,\,\sigma_u$. We can (to first order, assuming $\delta x$ is small) write:
\begin{align}
  \delta x = f(u+\delta u) - f(u).
\end{align}
We can then write:
\begin{align}
  \frac{\delta x}{\delta u} = \frac{f(u+\delta u) - f(u)}{\delta u} = \frac{df}{du}\rightarrow \delta x = \left|\frac{df}{du}\right|\delta u,
\end{align}
or:
\begin{align}
  \sigma_x = \left|\frac{df}{du}\right|\sigma_u
\end{align}
Therefore, the measurement related to equation~\ref{eq:x_func} would be of the form:
\begin{align}
  \text{measured value} = x\pm|2u|\sigma_u
\end{align}
An example of this would be the area of a square with side length $u$, and an uncertainty in the ruler used to measure the side which is $\sigma_u$. 

\noindent Now let's expand this to an arbitrary number of independent variables. Let's take $x$ in the form of $f$ again:
\begin{align}
  x = f(u,v,w,...),
\end{align}
it follows that:
\begin{align}
  \overline{x} = f(\overline{u},\overline{v},\overline{w},...) \hspace{1cm} x_i = f(u_i,v_i,w_i,...)
\end{align}
We can write:
\begin{dmath}
  x_i - \overline{x} \cong (u_i - \overline{u})\left(\frac{\partial x}{\partial u}\right)_{v,w,...} + (v_i - \overline{v})\left(\frac{\partial x}{\partial v}\right)_{u,w,...} + ...
  \label{eq:iminusbar}
\end{dmath}
Now recall equation~\ref{eq:variance}, we can write:
\begin{dmath}
  \sigma_x^2 = \lim_{N\rightarrow\infty}\frac{1}{N}\sum_i^N\biggl[(u_i - \overline{u})^2\left(\frac{\partial x}{\partial u}\right)^2 + (v_i -\overline{v})^2\left(\frac{\partial x}{\partial v}\right)^2 + ... + \underbrace{2(u_i - \overline{u})(v_i - \overline{v})\left(\frac{\partial x}{\partial u}\right)\left(\frac{\partial x}{\partial v}\right)+...}_{\text{cross terms from squaring eq.~\ref{eq:iminusbar}}}\biggr]
  \label{eq:bigguy}
\end{dmath}
Now using our standard variance equation with the form:
\begin{align}
  \sigma_u^2 = \lim_{N\rightarrow\infty}\frac{1}{N}\sum_i^N (u_i - \overline{u})^2,
\end{align}
and now introducing the covariance between $u$ and $v$:
\begin{align}
  \sigma_{uv}^2 = \lim_{N\rightarrow\infty}\frac{1}{N}\sum_i^N (u_i - \overline{u})(v_i - \overline{v}).
  \label{eq:covariance}
\end{align}
We can now rewrite equation~\ref{eq:bigguy} in the form:
\begin{align}
  \sigma_x^2 \cong \sigma_u^2\left(\frac{\partial x}{\partial u}\right) + \sigma_v^2\left(\frac{\partial x}{\partial v}\right) + ... + 2\sigma_{uv}^2\left(\frac{\partial x}{\partial u}\right)\left(\frac{\partial x}{\partial v}\right) + ... 
\end{align}
If $u$ and $v$ are independent of eachother, in the large $N$ limit, $\sigma_{uv}^2 \rightarrow 0$ and all of the ``cross-terms'' vanish, yielding a variance of:
\begin{align}
  \sigma_x^2 \cong \sigma_u^2\left(\frac{\partial x}{\partial u}\right) + \sigma_v^2\left(\frac{\partial x}{\partial v}\right) + ...
\end{align}
\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here is a citation, just for fun~\cite{CW87}.

\section*{References}
\beginrefs
\bibentry{CW87}{\sc D.~Coppersmith} and {\sc S.~Winograd}, 
``Matrix multiplication via arithmetic progressions,''
         {\it Proceedings of the 19th ACM Symposium on Theory of Computing},
         1987, pp.~1--6.
         \endrefs
         
